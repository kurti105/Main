{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c8257f5-7e69-45d5-809e-4a115643f8a4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In this notebook I will just be testing the code that will go into the `script.py` - it is purely for experimental reasons, and it will not be needed elsewhere\n",
    "\n",
    "Basically, in the `script.py` we will do the following:\n",
    "1. Import necessary libraries & get data\n",
    "2. Encode the categories\n",
    "3. Prepare the Tokenization for the input text (i.e., the news headlines in our case)\n",
    "4. Create the PyTorch Dataset class - this is where we will tokenize the text data\n",
    "5. Split the data into training & testing\n",
    "6. Create the training/testing PyTorch DataLoaders\n",
    "7. Create the model class & initialize a model\n",
    "8. Select loss function, optimizer, accuracy metrics - this is just for clarity of process, we will actually select them in step 10\n",
    "9. Create training loop\n",
    "10. Set up the `main()` function which will start everything"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323b9b65-ac20-4b13-9bd6-24b3c45629d0",
   "metadata": {},
   "source": [
    "# 1. Import libraries & modules, and get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73129bfb-55b8-4130-8d23-bcaecf779803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: s3fs in /opt/conda/lib/python3.11/site-packages (2024.12.0)\n",
      "Requirement already satisfied: aiobotocore<3.0.0,>=2.5.4 in /opt/conda/lib/python3.11/site-packages (from s3fs) (2.13.3)\n",
      "Requirement already satisfied: fsspec==2024.12.0.* in /opt/conda/lib/python3.11/site-packages (from s3fs) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.11/site-packages (from s3fs) (3.9.5)\n",
      "Requirement already satisfied: botocore<1.34.163,>=1.34.70 in /opt/conda/lib/python3.11/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.34.162)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /opt/conda/lib/python3.11/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.17.0)\n",
      "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /opt/conda/lib/python3.11/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (0.12.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.18.3)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from botocore<1.34.163,>=1.34.70->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.11/site-packages (from botocore<1.34.163,>=1.34.70->aiobotocore<3.0.0,>=2.5.4->s3fs) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.11/site-packages (from botocore<1.34.163,>=1.34.70->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.26.19)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.11/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (3.10)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (0.2.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.34.163,>=1.34.70->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69b9e88a-dd49-47c2-af19-676a46922ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in /opt/conda/lib/python3.11/site-packages (1.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33823b69-7dc2-4be1-a1d4-29513d359bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-18 07:18:16.163813: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "import torchmetrics\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchinfo import summary\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Dict, Tuple\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b110773e-ff14-43c2-8950-4d21d8a53edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the dataframe:\n",
      "                                               TITLE CATEGORY\n",
      "0  Fed official says weak data caused by weather,...        b\n",
      "1  Fed's Charles Plosser sees high bar for change...        b\n",
      "2  US open: Stocks fall after Fed official hints ...        b\n",
      "3  Fed risks falling 'behind the curve', Charles ...        b\n",
      "4  Fed's Plosser: Nasty Weather Has Curbed Job Gr...        b\n"
     ]
    }
   ],
   "source": [
    "# Let's get data\n",
    "s3_path = 's3://tk5-huggingface-multiclass-textclassification-bucket/training_data/newsCorpora.csv'\n",
    "\n",
    "df = pd.read_csv(s3_path, \n",
    "                 sep='\\t', \n",
    "                 names=['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP'])\n",
    "\n",
    "print(f\"First few rows of the dataframe:\\n{df[['TITLE', 'CATEGORY']].head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2319aabd-98be-4863-89c2-f92d66526d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's update the CATEGORY variable as we did before\n",
    "\n",
    "# Create the mapping through which we will update the CATEGORY variable\n",
    "my_dict = {\n",
    "    'b': 'BUSINESS',\n",
    "    't': 'SCIENCE',\n",
    "    'e': 'ENTERTAINMENT',\n",
    "    'm': 'HEALTH'\n",
    "}\n",
    "\n",
    "# Create helper function\n",
    "def update_category(x, dictionary: dict):\n",
    "    return dictionary.get(x)\n",
    "\n",
    "# Update the CATEGORY variable\n",
    "df['CATEGORY'] = df['CATEGORY'].apply(lambda x: update_category(x, dictionary=my_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "903a32be-df0d-4571-bcb3-2339ffe3cbd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>CATEGORY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fed official says weak data caused by weather,...</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fed's Charles Plosser sees high bar for change...</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US open: Stocks fall after Fed official hints ...</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fed risks falling 'behind the curve', Charles ...</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fed's Plosser: Nasty Weather Has Curbed Job Gr...</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               TITLE  CATEGORY\n",
       "0  Fed official says weak data caused by weather,...  BUSINESS\n",
       "1  Fed's Charles Plosser sees high bar for change...  BUSINESS\n",
       "2  US open: Stocks fall after Fed official hints ...  BUSINESS\n",
       "3  Fed risks falling 'behind the curve', Charles ...  BUSINESS\n",
       "4  Fed's Plosser: Nasty Weather Has Curbed Job Gr...  BUSINESS"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['TITLE', 'CATEGORY']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eef94671-2466-43b6-bb19-231bfe0cba9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "422419"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90835afd-4b5d-4e68-8e29-a765b331d6fe",
   "metadata": {},
   "source": [
    "Also, we will only train the model on a fraction of the entire data. The reason is that first I want to make sure that everything is running as intended, before we use the entire data and incur higher costs. I don't want to find out that there's something wrong with my code after having used a GPU instance for hours and hours!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c496480c-8339-46a8-a3ee-550b794d4c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=0.10, random_state=1) # selecting only 10% of the data\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03eae998-5483-4d45-984e-7678af492091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42242"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c45732b1-3c83-428a-8ca1-5e771fa35087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>CATEGORY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Murdoch's bid for Time Warner rejected</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rescuers close in on 3 trapped Honduran miners...</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Johnny Depp - Johnny Depp Served With Legal Pa...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apple prepping move into \"smart home\" connecti...</td>\n",
       "      <td>SCIENCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ripped First Look: Dwayne Johnson as Brett Rat...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               TITLE       CATEGORY\n",
       "0             Murdoch's bid for Time Warner rejected       BUSINESS\n",
       "1  Rescuers close in on 3 trapped Honduran miners...       BUSINESS\n",
       "2  Johnny Depp - Johnny Depp Served With Legal Pa...  ENTERTAINMENT\n",
       "3  Apple prepping move into \"smart home\" connecti...        SCIENCE\n",
       "4  Ripped First Look: Dwayne Johnson as Brett Rat...  ENTERTAINMENT"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98c16735-aead-4de2-956c-d77cd45330f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count by category:                TITLE\n",
      "CATEGORY            \n",
      "BUSINESS       11438\n",
      "ENTERTAINMENT  15275\n",
      "HEALTH          4566\n",
      "SCIENCE        10963\n"
     ]
    }
   ],
   "source": [
    "print(f\"Count by category: {df.groupby(['CATEGORY']).count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b8959a-459a-4afb-9583-d6b1fd523a31",
   "metadata": {},
   "source": [
    "# 2. Encode the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1842c728-38ca-4edf-a9f1-67b7b2a46807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>ENCODE_CAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Murdoch's bid for Time Warner rejected</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rescuers close in on 3 trapped Honduran miners...</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Johnny Depp - Johnny Depp Served With Legal Pa...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apple prepping move into \"smart home\" connecti...</td>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ripped First Look: Dwayne Johnson as Brett Rat...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               TITLE       CATEGORY  \\\n",
       "0             Murdoch's bid for Time Warner rejected       BUSINESS   \n",
       "1  Rescuers close in on 3 trapped Honduran miners...       BUSINESS   \n",
       "2  Johnny Depp - Johnny Depp Served With Legal Pa...  ENTERTAINMENT   \n",
       "3  Apple prepping move into \"smart home\" connecti...        SCIENCE   \n",
       "4  Ripped First Look: Dwayne Johnson as Brett Rat...  ENTERTAINMENT   \n",
       "\n",
       "   ENCODE_CAT  \n",
       "0           0  \n",
       "1           0  \n",
       "2           1  \n",
       "3           3  \n",
       "4           1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ENCODE_CAT'] = df.groupby(by=['CATEGORY']).ngroup() # the .ngroup() assigns a number to each unique category\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a4bfc52-7f82-45bb-829d-49f5f7a8e18f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>ENCODE_CAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Murdoch's bid for Time Warner rejected</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Johnny Depp - Johnny Depp Served With Legal Pa...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apple prepping move into \"smart home\" connecti...</td>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Some Mangos Sold In New Jersey Recalled</td>\n",
       "      <td>HEALTH</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                TITLE       CATEGORY  \\\n",
       "0              Murdoch's bid for Time Warner rejected       BUSINESS   \n",
       "2   Johnny Depp - Johnny Depp Served With Legal Pa...  ENTERTAINMENT   \n",
       "3   Apple prepping move into \"smart home\" connecti...        SCIENCE   \n",
       "15            Some Mangos Sold In New Jersey Recalled         HEALTH   \n",
       "\n",
       "    ENCODE_CAT  \n",
       "0            0  \n",
       "2            1  \n",
       "3            3  \n",
       "15           2  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quickly validate that our code is producing outputs as intended\n",
    "df.drop_duplicates(subset=['CATEGORY', 'ENCODE_CAT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e016d737-be11-4184-9099-13c4d5968a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>ENCODE_CAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Murdoch's bid for Time Warner rejected</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Johnny Depp - Johnny Depp Served With Legal Pa...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Some Mangos Sold In New Jersey Recalled</td>\n",
       "      <td>HEALTH</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apple prepping move into \"smart home\" connecti...</td>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               TITLE       CATEGORY  \\\n",
       "0             Murdoch's bid for Time Warner rejected       BUSINESS   \n",
       "1  Johnny Depp - Johnny Depp Served With Legal Pa...  ENTERTAINMENT   \n",
       "2            Some Mangos Sold In New Jersey Recalled         HEALTH   \n",
       "3  Apple prepping move into \"smart home\" connecti...        SCIENCE   \n",
       "\n",
       "   ENCODE_CAT  \n",
       "0           0  \n",
       "1           1  \n",
       "2           2  \n",
       "3           3  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_sample_df = df.drop_duplicates(subset=['CATEGORY', 'ENCODE_CAT'])\n",
    "small_sample_df = small_sample_df.sort_values(by=['ENCODE_CAT']).reset_index(drop=True)\n",
    "small_sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e228aa9-6388-4837-bf8d-5fb0a103d65d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'BUSINESS': 0, 'ENTERTAINMENT': 1, 'HEALTH': 2, 'SCIENCE': 3},\n",
       " ['BUSINESS', 'ENTERTAINMENT', 'HEALTH', 'SCIENCE'],\n",
       " [0, 1, 2, 3])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = list(small_sample_df['CATEGORY'])\n",
    "encoded_cats = list(small_sample_df['ENCODE_CAT'])\n",
    "categories, encoded_cats\n",
    "\n",
    "class_to_idx = {}\n",
    "i = 0\n",
    "for category in categories:\n",
    "    class_to_idx[f'{category}'] = encoded_cats[i]\n",
    "    i += 1\n",
    "\n",
    "class_to_idx, categories, encoded_cats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d076aab-57c2-47df-8ee8-2eccf05b1f22",
   "metadata": {},
   "source": [
    "# 3. Prepare the Text Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52f429f6-1c95-4c6f-91f4-68c3e605a435",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "# Get the tokenizer of choice\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6385be34-2111-4678-9a8b-c555190652fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1045, 2293, 2374, 1998, 3586, 1011, 5559, 102, 1045, 2253, 2000, 1996, 2118, 1997, 4361, 102, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize input data - just an example!\n",
    "inputs = tokenizer.encode_plus(\n",
    "    \"I love football and horse-riding\",\n",
    "    \"I went to the University of Toronto\",\n",
    "    add_special_tokens=True, # adds the ['CLS'] and ['SEP'] tokens\n",
    "    max_length=20,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_token_type_ids=True, # returns 0s for the first sentence, 1s for the second sentence and so on. Returns 0s when padding begins. If input is only one sentence & padding, it's all 0s \n",
    "    return_attention_mask=True # Specifies where the model should pay attention (1s) and where padding begins (0s)\n",
    ")\n",
    "\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fe26720-50ba-44ac-9206-8368c660dddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: [101, 1045, 2293, 2374, 1998, 3586, 1011, 5559, 102, 1045, 2253, 2000, 1996, 2118, 1997, 4361, 102, 0, 0, 0]\n",
      "Token Type IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input IDs: {inputs['input_ids']}\")\n",
    "print(f\"Token Type IDs: {inputs['token_type_ids']}\")\n",
    "print(f\"Attention Mask: {inputs['attention_mask']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba717d9-8664-478e-b78b-09ec10896969",
   "metadata": {},
   "source": [
    "# 4. Create the PyTorch Dataset class\n",
    "\n",
    "Here, we will prepare the data such that the model can learn from it - remember, the model cannot process text data directly, first we need to tokenize it (i.e., convert it into numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e26b4c7e-7f10-46ae-99cc-b3f766e6bc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "\n",
    "    # Define the __init__() method:\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Initialize the data, tokenizer & allocated maximum length for the model inputs (remember, our task deals with news headlines, so will choose max_length accordingly)\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.len = len(data)\n",
    "\n",
    "    # Define the __getitem__ method:\n",
    "    def __getitem__(self, index):\n",
    "        # 1. Get headline from the source dataframe\n",
    "        # headline = self.data.TITLE[index]\n",
    "        headline = str(self.data.iloc[index, 0]) # more efficient than the line above\n",
    "        headline = \" \".join(headline.split())\n",
    "\n",
    "        # 2. Tokenize the headline\n",
    "        headline_tokenized = self.tokenizer.encode_plus(\n",
    "            \n",
    "            headline, # input text\n",
    "            add_special_tokens = True,\n",
    "            max_length = self.max_length,\n",
    "            padding = 'max_length',\n",
    "            truncation = True,\n",
    "            return_attention_mask = True,\n",
    "            return_token_type_ids = True,\n",
    "        \n",
    "        )\n",
    "\n",
    "        ids = headline_tokenized['input_ids']\n",
    "        mask = headline_tokenized['attention_mask']\n",
    "        \n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.data.iloc[index, 2], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    # Define the __len__() method:\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795add6e-11a0-4288-a558-575f969fd419",
   "metadata": {},
   "source": [
    "# 5. Split the data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f852e9a1-cf48-4a51-bfda-1ac6d5073a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True, stratify=df['CATEGORY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9d3e256c-3e9a-4adf-aa7b-72cd7ab52b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data has shape: (33793, 3)\n",
      "Testing data has shape: (8449, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training data has shape: {train_df.shape}\")\n",
    "print(f\"Testing data has shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed198a35-e407-45cb-ad0e-d81c607b346b",
   "metadata": {},
   "source": [
    "# 6. Create the Training/Testing PyTorch DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f688d131-24e6-4031-b32b-39a491fa9caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fc480d20-a8ca-44b0-8585-c7d6bc7e4272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33793"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = NewsDataset(data=train_df, tokenizer=tokenizer, max_length=MAX_LEN)\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d2cee1a3-33c6-4297-9193-aecc0b9fbc80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8449"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = NewsDataset(data=test_df, tokenizer=tokenizer, max_length=MAX_LEN)\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "188888d9-d73b-474b-ab6a-0eb3922c0523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DataLoader\n",
    "train_dataloader = DataLoader(dataset=train_dataset,\n",
    "                              batch_size = TRAIN_BATCH_SIZE,\n",
    "                              shuffle=True, \n",
    "                              num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b3c56ab8-3313-48e7-998c-019dea0dc1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test DataLoader\n",
    "test_dataloader = DataLoader(dataset=test_dataset, \n",
    "                             batch_size=VALID_BATCH_SIZE, \n",
    "                             shuffle=False, \n",
    "                             num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd7eacc-4193-4426-8de5-2327bda57d08",
   "metadata": {},
   "source": [
    "# 7. Create the model class\n",
    "\n",
    "Here we will adapt the DistilBERT architecture to our own specific problem - we will be adding additional layers to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "11cb4343-b0fe-428e-82d2-f2d4cc461fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FT_DistilBERT(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.block_1 = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.layer_2 = nn.Linear(in_features=768,\n",
    "                                 out_features=768)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.classifier_layer = nn.Linear(in_features=768,\n",
    "                                          out_features=num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, mask_ids):\n",
    "        # 1. Send through the DistilBERT pre-trained model\n",
    "        output = self.block_1(input_ids = input_ids,\n",
    "                              attention_mask = mask_ids)\n",
    "        hidden_state = output[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        \n",
    "        # 2. Send through the linear layer - this serves to increase the representational capacity of our model\n",
    "        output = self.layer_2(pooler)\n",
    "        # 3. Send through a non-linear activation function\n",
    "        output = self.activation(output)\n",
    "        # 4. Apply dropout to fight over-fitting\n",
    "        output = self.dropout(output)\n",
    "        # 5. Get the classification prediction (in logits)\n",
    "        output = self.classifier_layer(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34d55e0c-c7d0-44fe-83f4-ac693fb3b15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = FT_DistilBERT(num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "914fa5ae-2a01-4353-9720-36f604859185",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape          Output Shape         Trainable            Param #\n",
       "============================================================================================================================================\n",
       "FT_DistilBERT (FT_DistilBERT)                                --                   [1, 4]               True                 --\n",
       "├─DistilBertModel (block_1)                                  --                   [1, 512, 768]        True                 --\n",
       "│    └─Embeddings (embeddings)                               [1, 512]             [1, 512, 768]        True                 --\n",
       "│    │    └─Embedding (word_embeddings)                      [1, 512]             [1, 512, 768]        True                 23,440,896\n",
       "│    │    └─Embedding (position_embeddings)                  [1, 512]             [1, 512, 768]        True                 393,216\n",
       "│    │    └─LayerNorm (LayerNorm)                            [1, 512, 768]        [1, 512, 768]        True                 1,536\n",
       "│    │    └─Dropout (dropout)                                [1, 512, 768]        [1, 512, 768]        --                   --\n",
       "│    └─Transformer (transformer)                             --                   [1, 512, 768]        True                 --\n",
       "│    │    └─ModuleList (layer)                               --                   --                   True                 42,527,232\n",
       "├─Linear (layer_2)                                           [1, 768]             [1, 768]             True                 590,592\n",
       "├─ReLU (activation)                                          [1, 768]             [1, 768]             --                   --\n",
       "├─Dropout (dropout)                                          [1, 768]             [1, 768]             --                   --\n",
       "├─Linear (classifier_layer)                                  [1, 768]             [1, 4]               True                 3,076\n",
       "============================================================================================================================================\n",
       "Total params: 66,956,548\n",
       "Trainable params: 66,956,548\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 66.96\n",
       "============================================================================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 217.06\n",
       "Params size (MB): 267.83\n",
       "Estimated Total Size (MB): 484.90\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's view a summary of our model\n",
    "summary(model=model, \n",
    "        input_data={\"input_ids\": torch.randint(low=0, high=25, size=(1, 512), dtype=torch.long), \"mask_ids\":torch.ones(size=(1, 512), dtype=torch.long)},\n",
    "        col_names=['input_size', 'output_size', 'trainable', 'num_params'], \n",
    "        row_settings=['var_names'], \n",
    "        col_width=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343ef0e1-722d-42fd-ae07-42cf556c95e4",
   "metadata": {},
   "source": [
    "# 8. Select loss function, optimizer, and accuracy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0603043-10a1-406c-b228-260fcd92616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b6f11b02-98f1-4d4e-8681-d426fa4f0251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "LEARNING_RATE = 1e-05\n",
    "optimizer = torch.optim.Adam(params=model.parameters(),\n",
    "                             lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "915f3c07-2660-4164-9633-42adb8de823d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy metrics: Accuracy, Confusion Matrix\n",
    "accuracy_fn = torchmetrics.Accuracy(task='multiclass', num_classes=4)\n",
    "# conf_matrix = torchmetrics.ConfusionMatrix(task='multiclass', num_classes=4)\n",
    "precision = torchmetrics.Precision(task='multiclass', num_classes=4)\n",
    "recall = torchmetrics.Recall(task='multiclass', num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "47abbdfc-a8ad-4bd2-a7fc-008df79a7c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot the confusion matrix, will install mlxtend\n",
    "# !pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3ca0734c-3e6f-4ed9-800e-72417a445f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlxtend has a good-looking function to visualize the confusion matrix with color scaling\n",
    "# import mlxtend "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d887b837-63a8-480c-a826-2f755da6c9e3",
   "metadata": {},
   "source": [
    "# 9. Set up the Training Loop\n",
    "\n",
    "Here I will do the following:\n",
    "1. Create a train step\n",
    "2. Create a test/validation step\n",
    "3. Create a combined training loop that involves both steps 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8dbf1454-e93a-42c6-b558-c4ea080a6973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[MulticlassAccuracy(), MulticlassPrecision(), MulticlassRecall()]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_of_choice = [accuracy_fn, precision, recall]\n",
    "metrics_of_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "54b479ef-12d4-4c65-9dee-a15792bc0e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Train step\n",
    "\n",
    "def train_step(model: torch.nn.Module, \n",
    "               train_dataloader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               optimizer: torch.optim, \n",
    "               metrics: List[torchmetrics], \n",
    "               device: torch.device):\n",
    "    \"\"\"\n",
    "    This function conducts one training loop across all batches in a DataLoader\n",
    "\n",
    "    Parameters:\n",
    "        - model: a PyTorch model architecture that will be trained\n",
    "        - train_dataloader: a PyTorch DataLoader that contains the training batches\n",
    "        - loss_fn: a PyTorch loss function through which we measure how much the model errors\n",
    "        - optimizer: a PyTorch optimizer that determines how we adjust the weights of the model\n",
    "        - metrics: a list of PyTorch (torchmetrics) metrics that we want to review as the model progresses through training\n",
    "        - device: the target device in which we will train the model (e.g., CPU, GPU)\n",
    "    \"\"\"\n",
    "    # 0. Set up variables that will contain the training loss, and accuracy metric\n",
    "    total_train_loss = 0\n",
    "    total_train_acc = 0\n",
    "    total_train_prec = 0\n",
    "    total_train_rec = 0\n",
    "    \n",
    "    ## I also want to keep a record of how the loss & accuracy metrics develop per each batch in an epoch - in case we want to plot the progress\n",
    "    results_dict = {\n",
    "        'batch_train_loss': [],\n",
    "        'batch_train_acc': [],\n",
    "        'batch_train_prec': [],\n",
    "        'batch_train_rec': []\n",
    "    } \n",
    "    \n",
    "    \n",
    "    # 1. Set the model in training mode\n",
    "    model.train()\n",
    "\n",
    "    # 2. Send model to target device\n",
    "    model.to(device)\n",
    "\n",
    "    # 3. Start the training step\n",
    "    for idx, data in enumerate(train_dataloader):\n",
    "        \n",
    "        # 3.1 Get data into target device\n",
    "        inputs = data['ids'].to(device)\n",
    "        mask = data['mask'].to(device)\n",
    "        targets = data['targets'].to(device)\n",
    "\n",
    "        # 3.2 Run model on data\n",
    "        outputs = model(input_ids=inputs,\n",
    "                        mask_ids=mask)\n",
    "        probabilities = torch.softmax(outputs,\n",
    "                                      dim=1)\n",
    "        predictions = torch.argmax(probabilities,\n",
    "                                   dim=1)\n",
    "\n",
    "        # 3.3 Calculate the loss & accuracy metrics\n",
    "        ## Loss\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        total_train_loss += loss\n",
    "        results_dict['batch_train_loss'].append(loss) \n",
    "\n",
    "        ## Metrics\n",
    "        for metric in metrics:\n",
    "            \n",
    "            if metric == MulticlassAccuracy():\n",
    "                accuracy = metric(predictions, targets)\n",
    "                total_train_acc += accuracy\n",
    "                results_dict['batch_train_acc'].append(accuracy)\n",
    "            \n",
    "            elif metric == MulticlassPrecision():\n",
    "                precision = metric(predictions, targets)\n",
    "                total_train_prec += precision\n",
    "                results_dict['batch_train_prec'].append(precision)\n",
    "            \n",
    "            elif metric == MulticlassRecall():\n",
    "                recall = metric(predictions, targets)\n",
    "                total_train_rec += recall\n",
    "                results_dict['batch_train_rec'].append(recall)\n",
    "\n",
    "        \n",
    "        # 3.4 Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 3.5 Do backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # 3.6 Apply optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # Print out what's happening every 100th batch - we get the loss & metrics results per batch by dividing by the number of batches:\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"Training loss as at the {idx}-th batch: {total_train_loss/(idx+1):.3f}\\n\"\n",
    "                  f\"Train accuracy as at the {idx}-th batch: {total_train_acc/(idx+1)*100:.2f}%\\n\"\n",
    "                  f\"Train precision as at the {idx}-th batch: {total_train_prec/(idx+1):.3f}\\n\"\n",
    "                  f\"Train recall as at the {idx}-th batch: {total_train_rec/(idx+1):.3f}\")\n",
    "            # print(f\"Train accuracy as at the {idx}-th batch: {train_acc/(idx+1)*100:.2f}%\")\n",
    "            # print(f\"Train precision as at the {idx}-th batch: {train_prec/(idx+1):.3f}\")\n",
    "            # print(f\"Train recall as at the {idx}-th batch: {train_rec/(idx+1):.3f}\")\n",
    "\n",
    "    # 4. Get train loss/accuracy/precision/recall per batch\n",
    "    total_train_acc = total_train_acc / len(train_dataloader)\n",
    "    total_train_prec = total_train_prec / len(train_dataloader)\n",
    "    total_train_rec = total_train_rec / len(train_dataloader)\n",
    "    \n",
    "    return total_train_loss, total_train_acc, total_train_prec, total_train_rec, results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "439fcc33-65a8-4296-aa60-608cef2435da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Test step\n",
    "\n",
    "def test_step(model, \n",
    "              test_dataloader: torch.utils.data.DataLoader, \n",
    "              loss_fn: torch.nn.Module, \n",
    "              metrics: List[torchmetrics], \n",
    "              device: torch.device):\n",
    "    \"\"\"\n",
    "    This function conducts one validation loop across all batches in a DataLoader\n",
    "\n",
    "    Parameters:\n",
    "        - model: a PyTorch model architecture that will be trained\n",
    "        - test_dataloader: a PyTorch DataLoader that contains the training batches\n",
    "        - loss_fn: a PyTorch loss function through which we measure how much the model errors\n",
    "        - metrics: a list of PyTorch (torchmetrics) metrics that we want to review as the model progresses through training\n",
    "        - device: the target device in which we will train the model (e.g., CPU, GPU)\n",
    "    \"\"\"\n",
    "    # 0. Set up variables that will contain the training loss, and accuracy metric\n",
    "    total_test_loss = 0\n",
    "    total_test_acc = 0\n",
    "    total_test_prec = 0\n",
    "    total_test_rec = 0\n",
    "    \n",
    "    ## To keep a record of how the loss & accuracy metrics develop per each batch in an epoch\n",
    "    results_dict = {\n",
    "        'batch_test_loss': [],\n",
    "        'batch_test_acc': [],\n",
    "        'batch_test_prec': [],\n",
    "        'batch_test_rec': []\n",
    "    } \n",
    "    \n",
    "    # 1. Set the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # 2. Send model to target device\n",
    "    model.to(device)\n",
    "\n",
    "    \n",
    "    # 3. Start validation loop\n",
    "    \n",
    "    ## 3.1 Set model in inference mode\n",
    "    with torch.inference_mode():\n",
    "\n",
    "        for idx, data in enumerate(test_dataloader):\n",
    "            \n",
    "            ## 3.2 Get data into target device\n",
    "            inputs = data['ids'].to(device)\n",
    "            mask = data['mask'].to(device)\n",
    "            targets = data['targets'].to(device)\n",
    "\n",
    "            ## 3.3 Get predictions\n",
    "            outputs = model(input_ids=inputs,\n",
    "                            mask_ids=mask)\n",
    "            probabilities = torch.softmax(outputs,\n",
    "                                          dim=1)\n",
    "            predictions = torch.argmax(probabilities,\n",
    "                                       dim=1)\n",
    "\n",
    "            ## 3.4 Estimate loss & metrics\n",
    "            ### Loss\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            total_test_loss += loss\n",
    "            results_dict['batch_test_loss'].append(loss) \n",
    "    \n",
    "            ### Metrics\n",
    "            for metric in metrics:\n",
    "                \n",
    "                if metric == MulticlassAccuracy():\n",
    "                    accuracy = metric(predictions, targets)\n",
    "                    total_test_acc += accuracy\n",
    "                    results_dict['batch_test_acc'].append(accuracy)\n",
    "                \n",
    "                elif metric == MulticlassPrecision():\n",
    "                    precision = metric(predictions, targets)\n",
    "                    total_test_prec += precision\n",
    "                    results_dict['batch_test_prec'].append(precision)\n",
    "                \n",
    "                elif metric == MulticlassRecall():\n",
    "                    recall = metric(predictions, targets)\n",
    "                    total_test_rec += recall\n",
    "                    results_dict['batch_test_rec'].append(recall)\n",
    "\n",
    "            # Print out what's happening every 100th batch - we get the loss & metrics results per batch by dividing by the number of batches:\n",
    "            if idx % 100 == 0:\n",
    "                print(f\"Testing loss as at the {idx}-th batch: {total_test_loss/(idx+1):.3f}\\n\"\n",
    "                      f\"Test accuracy as at the {idx}-th batch: {total_test_acc/(idx+1)*100:.2f}%\\n\"\n",
    "                      f\"Test precision as at the {idx}-th batch: {total_test_prec/(idx+1):.3f}\\n\"\n",
    "                      f\"Test recall as at the {idx}-th batch: {total_test_rec/(idx+1):.3f}\")\n",
    "\n",
    "    # 4. Get test loss/accuracy/precision/recall per batch\n",
    "    total_test_loss = total_test_loss / len(test_dataloader)\n",
    "    total_test_acc = total_test_acc / len(test_dataloader)\n",
    "    total_test_prec = total_test_prec / len(test_dataloader)\n",
    "    total_test_rec = total_test_rec / len(test_dataloader)\n",
    "\n",
    "    return total_test_loss, total_test_acc, total_test_prec, total_test_rec, results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bbaaa598-7d65-46ce-92e6-619c1571fc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Consolidated Training Loop\n",
    "\n",
    "def train(model: torch.nn.Module,\n",
    "          train_dataloader: torch.utils.data.DataLoader, \n",
    "          test_dataloader: torch.utils.data.DataLoader, \n",
    "          epochs: int, \n",
    "          loss_fn: torch.nn.Module, \n",
    "          optimizer: torch.optim, \n",
    "          metrics: List[torchmetrics],\n",
    "          device: torch.device):\n",
    "    \"\"\"\n",
    "    This function combines the train_step() and test_step() functions we created above, to provide a consolidated training loop that includes both training & validation for a \n",
    "    specified number of epochs.\n",
    "\n",
    "    Arguments:\n",
    "        - model: a PyTorch model architecture that will be trained\n",
    "        - train_dataloader: a PyTorch DataLoader that contains the training batches\n",
    "        - test_dataloader: a PyTorch DataLoader that contains the testing batches\n",
    "        - epochs: the number of epochs for which we will train the model (i.e., how many full iterations through the train & test DataLoaders)\n",
    "        - loss_fn: a PyTorch loss function through which we measure how much the model errors\n",
    "        - optimizer: a PyTorch optimizer that determines how we adjust the weights of the model\n",
    "        - metrics: a list of PyTorch (torchmetrics) metrics that we want to review as the model progresses through training\n",
    "        - device: the target device in which we will train the model (e.g., CPU, GPU)\n",
    "    \"\"\"\n",
    "    consolidated_results_dict = {}\n",
    "\n",
    "    # Get start time for model - want to check the time it takes to train, although SageMaker measures it as well (just for comparison)\n",
    "    train_start_time = timer()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1} begins\")\n",
    "        \n",
    "        # Training loop\n",
    "        train_loss_epoch, train_acc_epoch, train_prec_epoch, train_rec_epoch, consolidated_results_dict[f'Epoch {epoch} training results'] = train_step(model=model,\n",
    "                                                                                                                                                        train_dataloader=train_dataloader,\n",
    "                                                                                                                                                        loss_fn=loss_fn,\n",
    "                                                                                                                                                        optimizer=optimizer,\n",
    "                                                                                                                                                        metrics=metrics,\n",
    "                                                                                                                                                        device=device)\n",
    "        \n",
    "\n",
    "        # Testing loop\n",
    "        test_loss_epoch, test_acc_epoch, test_prec_epoch, test_rec_epoch, consolidated_results_dict[f'Epoch {epoch} testing results'] = test_step(model=model,\n",
    "                                                                                                                                                  test_dataloader=test_dataloader,\n",
    "                                                                                                                                                  loss_fn=loss_fn,\n",
    "                                                                                                                                                  metrics=metrics,\n",
    "                                                                                                                                                  device=device)\n",
    "        \n",
    "        # Print out some results\n",
    "        print(f\"Epoch {epoch+1} ends - here are the results:\")\n",
    "        print(f\"Train Loss: {train_loss_epoch:.3f}\\nTrain Acc: {train_acc_epoch*100:.2f}%\\nTrain Precision: {train_prec_epoch:.3f}\\nTrain Recall: {train_rec_epoch:.3f}\")\n",
    "        print(f\"Test Loss: {test_loss_epoch:.3f}\\nTest Acc: {test_acc_epoch*100:.2f}%\\nTest Precision: {test_prec_epoch:.3f}\\nTest Recall: {test_rec_epoch:.3f}\")\n",
    "        print(f\"-\"*100)\n",
    "\n",
    "    # Get end time for model\n",
    "    train_end_time = timer()\n",
    "\n",
    "    print(f\"Time to train model: {train_end_time - train_start_time:.2f}/60 minutes\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26edc948-5173-41a1-93f5-52a3584574a8",
   "metadata": {},
   "source": [
    "# 10. Setup the `main()` function that will start everything!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2bb21dd3-3aec-45b5-9192-f9d96eb3719c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Start Training\")\n",
    "\n",
    "    # 1. Setup device-agnostic code\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # 2. Create argument parser that will provide the hyperparameters to the model\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    ## Argument for number of epochs\n",
    "    parser.add_argument(\"--epochs\", type=int, default=2)\n",
    "    ## Argument for train batch size\n",
    "    parser.add_argument(\"--train_batch_size\", type=int, default=4) # we have already created the dataloaders, so won't be using this - could have added everything here in the main() function\n",
    "    ## Argument for test batch size\n",
    "    parser.add_argument(\"--test_batch_size\", type=int, default=2) # we have already created the dataloaders, so won't be using this - could have added everything here in the main() function\n",
    "    ## Argument for the learning rate of the optimizer\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=5e-05)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # 3. Get tokenizer\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "    # 4. Initialize a model instance\n",
    "    model = FT_DistilBERT(num_classes=4)\n",
    "\n",
    "    # 5. Choose loss function, optimizer & accuracy metrics\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), \n",
    "                                 lr=args.learning_rate)\n",
    "    accuracy_fn = torchmetrics.Accuracy(task='multiclass', num_classes=4)\n",
    "    precision_fn = torchmetrics.Precision(task='multiclass', num_classes=4)\n",
    "    recall_fn = torchmetrics.Recall(task='multiclass', num_classes=4)\n",
    "    metrics_list = [accuracy_fn, precision_fn, recall_fn]\n",
    "\n",
    "    # 6. Train model\n",
    "    train(model=model,\n",
    "          train_dataloader=train_dataloader,\n",
    "          test_dataloader=test_dataloader,\n",
    "          epochs=args.epochs,\n",
    "          loss_fn=loss_fn,\n",
    "          optimizer=optimizer,\n",
    "          metrics=metrics_list,\n",
    "          device=device)\n",
    "\n",
    "    # 7. Specify output directory\n",
    "    output_dir = os.environ['SM_MODEL_DIR']\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "    output_model_file = os.path.join(output_dir, 'pytorch_distilbert_model_news.bin')\n",
    "    output_vocab_file = os.path.join(output_dir, 'vocab_distilbert_news.bin')\n",
    "\n",
    "    # 8. Save model weights & vocabulary\n",
    "    torch.save(obj=model.state_dict(),\n",
    "               f=output_model_file)\n",
    "    tokenizer.save_vocabulary(save_directory=output_vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35713273-12e2-4938-9b8c-32afe420feaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run the code:\n",
    "#if __name__ == '__main__':\n",
    "#    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc88c1c-5692-4d44-8eb4-b5c18ede02dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
